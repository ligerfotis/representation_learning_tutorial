{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b45e933be2198bd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# SimCLR Implementation and Evaluation on CIFAR-10\n",
    "\n",
    "This notebook implements the SimCLR algorithm, trains it on the CIFAR-10 dataset, and evaluates the learned representations using Linear Probing and K-Nearest Neighbors (KNN) classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3a06b9e367185d3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T15:32:59.891300108Z",
     "start_time": "2023-09-25T15:32:58.676550723Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "    \n",
    "\n",
    "# Importing necessary libraries and modules for the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67976416",
   "metadata": {},
   "source": [
    "### Choose the Contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59dbcfbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T15:32:59.894724706Z",
     "start_time": "2023-09-25T15:32:59.892001855Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Choose the contrastive loss: 'nt_xent' or 'contrastive'\n",
    "loss_choice = 'nt_xent'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b72739b",
   "metadata": {},
   "source": [
    "### Execution Timers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54d447ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T15:33:01.134787895Z",
     "start_time": "2023-09-25T15:33:01.123770067Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Flag to enable or disable timers\n",
    "enable_timers = True\n",
    "\n",
    "import time\n",
    "\n",
    "class Timer:\n",
    "    def __enter__(self):\n",
    "        if enable_timers:\n",
    "            self.start = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        if enable_timers:\n",
    "            self.end = time.time()\n",
    "            self.interval = self.end - self.start\n",
    "            print(f\"Elapsed time: {self.interval:.2f} seconds\")\n",
    "    \n",
    "\n",
    "# Importing necessary libraries and modules for the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddc28dacb40b4ef",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Define the data augmentation pipeline for SimCLR, including random cropping, color jittering, and random flipping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-25T15:33:02.512060240Z",
     "start_time": "2023-09-25T15:33:02.509812784Z"
    }
   },
   "outputs": [],
   "source": [
    "def simclr_augmentation(img_size):\n",
    "    color_jitter = transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=img_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomApply([color_jitter], p=0.8),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "cifar10_transform = simclr_augmentation(32)\n",
    "\n",
    "\n",
    "# Define a function for SimCLR data augmentation. This includes random resized cropping, random horizontal flip, color jittering, and grayscale conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b017aff08498197",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load CIFAR-10 Dataset\n",
    "\n",
    "Load the CIFAR-10 training and test datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "184d173ca4466895",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T15:35:09.120191806Z",
     "start_time": "2023-09-25T15:35:08.066624955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Elapsed time: 1.05 seconds\n"
     ]
    }
   ],
   "source": [
    "with Timer():\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=cifar10_transform)\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "    \n",
    "\n",
    "# Load the CIFAR-10 dataset. Training data will undergo the SimCLR augmentation while test data will only be transformed to tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50c4c48b88d94ba",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Define SimCLR Encoder and Projection Head\n",
    "\n",
    "Create the encoder model and projection head using ResNet18 as the base architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ee2abd1db6cba59",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T15:44:30.085768783Z",
     "start_time": "2023-09-25T15:44:29.968612411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.12 seconds\n"
     ]
    }
   ],
   "source": [
    "with Timer():\n",
    "    resnet = torchvision.models.resnet18(pretrained=False)\n",
    "    \n",
    "    class SimCLREncoder(nn.Module):\n",
    "        def __init__(self, base_encoder, projection_dim=128):\n",
    "            super(SimCLREncoder, self).__init__()\n",
    "            self.resnet = base_encoder\n",
    "            self.projection_head = nn.Sequential(\n",
    "                nn.Linear(1000, 512, bias=False),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, projection_dim, bias=False)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.resnet(x)\n",
    "            x = self.projection_head(x)\n",
    "            return x\n",
    "    \n",
    "    encoder = SimCLREncoder(resnet).to(\"cuda\")\n",
    "    \n",
    "\n",
    "# Define the SimCLR encoder which consists of a base encoder (ResNet18 in this case) and a projection head."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d149f22bdda4adaf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Define Contrastive Loss\n",
    "\n",
    "Implement the contrastive loss function used by SimCLR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96e4f7ee160bb2ee",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T15:44:30.360238226Z",
     "start_time": "2023-09-25T15:44:30.346586442Z"
    }
   },
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        norm_i = torch.norm(z_i, dim=1).reshape(-1, 1)\n",
    "        norm_j = torch.norm(z_j, dim=1).reshape(-1, 1)\n",
    "        z_i = z_i / norm_i\n",
    "        z_j = z_j / norm_j\n",
    "\n",
    "        sim_ij = torch.mm(z_i, z_j.T) / self.temperature\n",
    "        sim_ji = torch.mm(z_j, z_i.T) / self.temperature\n",
    "\n",
    "        loss_matrix = - torch.log_softmax(sim_ij, dim=1)\n",
    "        loss = loss_matrix.sum(dim=1).mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "# Define a generic contrastive loss. This computes the similarity between positive pairs and contrasts it with negative pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "577b5e81f10c8164",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T15:44:30.619007829Z",
     "start_time": "2023-09-25T15:44:30.585414458Z"
    }
   },
   "outputs": [],
   "source": [
    "class NTXentLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5, device=\"cuda\"):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.device = device\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        self.similarity_f = nn.CosineSimilarity(dim=2)\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        z = torch.cat([z_i, z_j], dim=0)\n",
    "        n = z.size(0)\n",
    "        device = z.device\n",
    "        # Compute similarity matrix\n",
    "        sim = self.similarity_f(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n",
    "\n",
    "        # Exclude the main diagonal from the similarity computation\n",
    "        mask = torch.eye(n, device=self.device).bool()\n",
    "        sim = sim.to(device)\n",
    "        mask = mask.to(device)\n",
    "        sim.masked_fill_(mask, float('-inf'))\n",
    "        positive_samples_i = sim[:n // 2, n // 2:].diag().unsqueeze(-1)\n",
    "        positive_samples_j = sim[n // 2:, :n // 2].diag().unsqueeze(-1)\n",
    "        positive_samples = torch.cat([positive_samples_i, positive_samples_j], dim=0)\n",
    "        negatives = sim.masked_select(mask == 0).reshape(n, -1)\n",
    "        \n",
    "        # Logits and labels for the loss\n",
    "        logits = torch.cat((positive_samples, negatives), dim=1)\n",
    "        logits /= self.temperature\n",
    "        # Symmetric labels: {0,...,0,1,...,1}\n",
    "        labels = torch.zeros(logits.shape[0], dtype=torch.long).to(self.device)\n",
    "        logits = logits.to(device)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss /= n\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Define the NT-Xent loss used in SimCLR. This loss contrasts the similarity of positive pairs with that of negative pairs and scales it by a temperature parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb5631261d0f6bd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training SimCLR\n",
    "\n",
    "Train the SimCLR model using the contrastive loss and augmented image pairs from CIFAR-10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# Create data loaders with SimCLR augmentations\n",
    "train_loader_i = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=4)\n",
    "train_loader_j = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T15:51:23.466219508Z",
     "start_time": "2023-09-25T15:51:23.453692341Z"
    }
   },
   "id": "bfad5b587765fc25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7f76864c04709f",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-09-25T15:51:24.111013579Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 6.93149:  16%|█▌        | 8/50 [01:54<09:06, 13.01s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "with Timer():\n",
    "    # Initialize optimizer and loss criterion\n",
    "    learning_rate = 1e-4\n",
    "    optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    sceduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader_i), eta_min=0, last_epoch=-1)\n",
    "    if loss_choice == 'nt_xent':\n",
    "        criterion = NTXentLoss()\n",
    "    else:\n",
    "        criterion = ContrastiveLoss()\n",
    "    \n",
    "    # Set number of training epochs\n",
    "    epochs = 50\n",
    "    \n",
    "    pbar = tqdm(range(epochs))\n",
    "    # Training loop\n",
    "    for epoch in pbar:\n",
    "        for (x_i, _), (x_j, _) in zip(train_loader_i, train_loader_j):\n",
    "            x_i = x_i.cuda()\n",
    "            x_j = x_j.cuda()\n",
    "            # Pass the inputs through the encoder to get embeddings\n",
    "            z_i = encoder(x_i)\n",
    "            z_j = encoder(x_j)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(z_i, z_j)\n",
    "            \n",
    "            # Backpropagation and optimization step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # update the progress bar with the loss value\n",
    "            pbar.set_description(f\"Loss: {loss.item():.5f}\")\n",
    "        sceduler.step()\n",
    "   \n",
    "# save the trained model\n",
    "torch.save(encoder.state_dict(), f\"simclr_encoder_e{epoch}_lr{learning_rate}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a6537003374718",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Linear Probing\n",
    "\n",
    "Evaluate the learned representations using Linear Probing. A linear classifier is trained on top of the frozen encoder and its accuracy is reported on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f11af0e54679cb8a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T15:38:12.782875162Z",
     "start_time": "2023-09-25T15:38:12.609120327Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (256x128 and 512x10)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 19\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m images, labels \u001B[38;5;129;01min\u001B[39;00m DataLoader(train_dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m---> 19\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlinear_probe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m         loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[1;32m     21\u001B[0m         optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m~/PycharmProjects/representation_learning_tutorial/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[26], line 9\u001B[0m, in \u001B[0;36mLinearProbe.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m      8\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(x)\n\u001B[0;32m----> 9\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclassifier\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/PycharmProjects/representation_learning_tutorial/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/representation_learning_tutorial/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (256x128 and 512x10)"
     ]
    }
   ],
   "source": [
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, encoder, num_classes=10):\n",
    "        super(LinearProbe, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "linear_probe = LinearProbe(encoder)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(linear_probe.classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Training the linear probe\n",
    "for epoch in range(epochs):\n",
    "    for images, labels in DataLoader(train_dataset, batch_size=256, shuffle=True):\n",
    "        outputs = linear_probe(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate linear probe\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in DataLoader(test_dataset, batch_size=256):\n",
    "        outputs = linear_probe(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f\"Linear Probe Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "\n",
    "# Initialize the optimizer (Adam in this case) and set the loss criterion based on the loss_choice flag. Then, train the SimCLR model using the chosen loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c5cfb988c4d047",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## K-Nearest Neighbors (KNN) Classification\n",
    "\n",
    "Evaluate the learned representations using KNN classification. For each sample in the test set, we find the 'K' nearest samples from the training set in the embedding space and assign the label based on a majority vote from these neighbors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9f63e28b6b5219",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with Timer():\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in DataLoader(train_dataset, batch_size=256):\n",
    "            features = encoder(images)\n",
    "            train_features.append(features.cpu().numpy())\n",
    "            train_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    train_features = np.concatenate(train_features, axis=0)\n",
    "    train_labels = np.concatenate(train_labels, axis=0)\n",
    "    \n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn_classifier.fit(train_features, train_labels)\n",
    "    \n",
    "    test_features = []\n",
    "    test_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in DataLoader(test_dataset, batch_size=256):\n",
    "            features = encoder(images)\n",
    "            test_features.append(features.cpu().numpy())\n",
    "            test_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    test_features = np.concatenate(test_features, axis=0)\n",
    "    test_labels = np.concatenate(test_labels, axis=0)\n",
    "    \n",
    "    knn_accuracy = knn_classifier.score(test_features, test_labels)\n",
    "    print(f\"KNN Accuracy: {100 * knn_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ac35483a6b66ba",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### GPU-based KNN Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea09b80868a7615f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GPUKNN:\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k\n",
    "        self.train_features = None\n",
    "        self.train_labels = None\n",
    "    \n",
    "    def fit(self, train_features, train_labels):\n",
    "        self.train_features = self._normalize(train_features)\n",
    "        self.train_labels = train_labels\n",
    "    \n",
    "    def predict(self, test_features):\n",
    "        test_features = self._normalize(test_features)\n",
    "        similarity_matrix = torch.mm(test_features, self.train_features.t())\n",
    "        _, top_k_indices = similarity_matrix.topk(self.k, dim=1, largest=True, sorted=True)\n",
    "        top_k_labels = torch.gather(self.train_labels.expand(test_features.size(0), -1), 1, top_k_indices)\n",
    "        predicted_labels = torch.mode(top_k_labels, dim=1).values\n",
    "        return predicted_labels\n",
    "    \n",
    "    def _normalize(self, x):\n",
    "        return x / x.norm(p=2, dim=1, keepdim=True)\n",
    "    \n",
    "\n",
    "# Define a GPU-based K-Nearest Neighbors (KNN) classifier. This classifier uses matrix multiplication to compute similarities between feature vectors, making it efficient on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6cafd3c603e02b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with Timer():\n",
    "    gpu_knn_classifier = GPUKNN(k=5)\n",
    "    gpu_knn_classifier.fit(torch.from_numpy(train_features).cuda(), torch.from_numpy(train_labels).cuda())\n",
    "    \n",
    "\n",
    "# Define a GPU-based K-Nearest Neighbors (KNN) classifier. This classifier uses matrix multiplication to compute similarities between feature vectors, making it efficient on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12fe7e9aa4e991a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with Timer():\n",
    "    test_features = []\n",
    "    test_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in DataLoader(test_dataset, batch_size=256):\n",
    "            features = encoder(images).cuda()\n",
    "            test_features.append(features.cpu().numpy())\n",
    "            test_labels.append(labels.cpu().numpy())\n",
    "            \n",
    "    test_features = np.concatenate(test_features, axis=0)\n",
    "    test_labels = np.concatenate(test_labels, axis=0)\n",
    "    \n",
    "    gpu_knn_accuracy = (gpu_knn_classifier.predict(torch.from_numpy(test_features).cuda()) == torch.from_numpy(test_labels).cuda()).float().mean().item()\n",
    "    print(f\"GPU KNN Accuracy: {100 * gpu_knn_accuracy:.2f}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657ec33c4163cd10",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eda63601ba27ce",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with Timer():\n",
    "    print(f\"Linear Probe Accuracy: {100 * correct / total:.2f}%\")\n",
    "    print(f\"KNN Accuracy: {100 * knn_accuracy:.2f}%\")\n",
    "    print(f\"GPU KNN Accuracy: {100 * gpu_knn_accuracy:.2f}%\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
