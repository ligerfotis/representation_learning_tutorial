{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "| Credentials |                                  |\n",
    "|----|----------------------------------|\n",
    "|Host | Montanuniversitaet Leoben        |\n",
    "|Web | https://cps.unileoben.ac.at      |\n",
    "|Mail | cps@unileoben.ac.at              |\n",
    "|Author | Fotios Lygerakis                 |\n",
    "|Corresponding Authors | fotios.lygerakis@unileoben.ac.at |\n",
    "|Last edited | 28.09.2023                       |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f329ff5ef12e1f5"
  },
  {
   "cell_type": "markdown",
   "id": "5b45e933be2198bd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# SimCLR Implementation and Evaluation on CIFAR-10\n",
    "\n",
    "This notebook implements the SimCLR algorithm, trains it on the CIFAR-10 dataset, and evaluates the learned representations using Linear Probing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3a06b9e367185d3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T14:44:03.659551210Z",
     "start_time": "2023-10-31T14:44:02.268218889Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "# Importing necessary libraries and modules for the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b72739b",
   "metadata": {},
   "source": [
    "### Execution Timers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54d447ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T14:44:03.660017473Z",
     "start_time": "2023-10-31T14:44:03.659170640Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Flag to enable or disable timers\n",
    "enable_timers = True\n",
    "\n",
    "import time\n",
    "\n",
    "class Timer:\n",
    "    def __enter__(self):\n",
    "        if enable_timers:\n",
    "            self.start = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        if enable_timers:\n",
    "            self.end = time.time()\n",
    "            self.interval = self.end - self.start\n",
    "            print(f\"Elapsed time: {self.interval:.2f} seconds\")\n",
    "    \n",
    "\n",
    "# Importing necessary libraries and modules for the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b017aff08498197",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load CIFAR-10 Dataset\n",
    "\n",
    "Load the CIFAR-10 training and test datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "184d173ca4466895",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T14:44:05.307395734Z",
     "start_time": "2023-10-31T14:44:03.659795507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from data_aug.contrastive_learning_dataset import ContrastiveLearningDataset\n",
    "\n",
    "dataset = ContrastiveLearningDataset(root_folder='data')\n",
    "train_dataset = dataset.get_dataset('cifar10_train', 2, type='moco')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True,\n",
    "                              num_workers=8, pin_memory=True, drop_last=True\n",
    "                              )\n",
    "memory_dataset = dataset.get_dataset('cifar10_memory', 2, type='moco')\n",
    "memory_loader = DataLoader(memory_dataset, batch_size=512, shuffle=False,\n",
    "                              num_workers=8, pin_memory=True, drop_last=False\n",
    "                              )\n",
    "test_dataset = dataset.get_dataset('cifar10_test', 2, type='moco')\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False,\n",
    "                              num_workers=8, pin_memory=True, drop_last=False\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "52453cbfe9cae13a"
  },
  {
   "cell_type": "markdown",
   "id": "f50c4c48b88d94ba",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Define SimCLR Encoder and Projection Head\n",
    "\n",
    "Create the encoder model and projection head using ResNet18 as the base architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ee2abd1db6cba59",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T14:44:05.307593301Z",
     "start_time": "2023-10-31T14:44:05.287917787Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet\n",
    "\n",
    "\n",
    "class ModelBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Common CIFAR ResNet recipe.\n",
    "    Comparing with ImageNet ResNet recipe, it:\n",
    "    (i) replaces conv1 with kernel=3, str=1\n",
    "    (ii) removes pool1\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim=128, arch=None, bn_splits=16):\n",
    "        super(ModelBase, self).__init__()\n",
    "\n",
    "        # use split batchnorm\n",
    "        # norm_layer = partial(SplitBatchNorm, num_splits=bn_splits) if bn_splits > 1 else nn.BatchNorm2d\n",
    "        norm_layer = nn.BatchNorm2d\n",
    "        resnet_arch = getattr(resnet, arch)\n",
    "        net = resnet_arch(num_classes=feature_dim, norm_layer=norm_layer)\n",
    "\n",
    "        self.net = []\n",
    "        for name, module in net.named_children():\n",
    "            if name == 'conv1':\n",
    "                module = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            if isinstance(module, nn.MaxPool2d):\n",
    "                continue\n",
    "            if isinstance(module, nn.Linear):\n",
    "                self.net.append(nn.Flatten(1))\n",
    "            self.net.append(module)\n",
    "\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        # note: not normalized here\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d149f22bdda4adaf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Define Contrastive Loss\n",
    "\n",
    "Implement the contrastive loss function used by SimCLR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "577b5e81f10c8164",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T14:44:05.307681446Z",
     "start_time": "2023-10-31T14:44:05.294287669Z"
    }
   },
   "outputs": [],
   "source": [
    "class ModelMoCo(nn.Module):\n",
    "    def __init__(self, dim=128, K=4096, m=0.99, T=0.1, arch='resnet18', bn_splits=8, symmetric=True):\n",
    "        super(ModelMoCo, self).__init__()\n",
    "\n",
    "        self.K = K\n",
    "        self.m = m\n",
    "        self.T = T\n",
    "        self.symmetric = symmetric\n",
    "\n",
    "        # create the encoders\n",
    "        self.encoder_q = ModelBase(feature_dim=dim, arch=arch, bn_splits=bn_splits)\n",
    "        self.encoder_k = ModelBase(feature_dim=dim, arch=arch, bn_splits=bn_splits)\n",
    "\n",
    "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
    "            param_k.data.copy_(param_q.data)  # initialize\n",
    "            param_k.requires_grad = False  # not update by gradient\n",
    "\n",
    "        # create the queue\n",
    "        self.register_buffer(\"queue\", torch.randn(dim, K))\n",
    "        self.queue = nn.functional.normalize(self.queue, dim=0)\n",
    "\n",
    "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _momentum_update_key_encoder(self):\n",
    "        \"\"\"\n",
    "        Momentum update of the key encoder\n",
    "        \"\"\"\n",
    "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
    "            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue(self, keys):\n",
    "        batch_size = keys.shape[0]\n",
    "\n",
    "        ptr = int(self.queue_ptr)\n",
    "        assert self.K % batch_size == 0  # for simplicity\n",
    "\n",
    "        # replace the keys at ptr (dequeue and enqueue)\n",
    "        self.queue[:, ptr:ptr + batch_size] = keys.t()  # transpose\n",
    "        ptr = (ptr + batch_size) % self.K  # move pointer\n",
    "\n",
    "        self.queue_ptr[0] = ptr\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _batch_shuffle_single_gpu(self, x):\n",
    "        \"\"\"\n",
    "        Batch shuffle, for making use of BatchNorm.\n",
    "        \"\"\"\n",
    "        # random shuffle index\n",
    "        idx_shuffle = torch.randperm(x.shape[0]).cuda()\n",
    "\n",
    "        # index for restoring\n",
    "        idx_unshuffle = torch.argsort(idx_shuffle)\n",
    "\n",
    "        return x[idx_shuffle], idx_unshuffle\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _batch_unshuffle_single_gpu(self, x, idx_unshuffle):\n",
    "        \"\"\"\n",
    "        Undo batch shuffle.\n",
    "        \"\"\"\n",
    "        return x[idx_unshuffle]\n",
    "\n",
    "    def contrastive_loss(self, im_q, im_k):\n",
    "        # compute query features\n",
    "        q = self.encoder_q(im_q)  # queries: NxC\n",
    "        q = nn.functional.normalize(q, dim=1)  # already normalized\n",
    "\n",
    "        # compute key features\n",
    "        with torch.no_grad():  # no gradient to keys\n",
    "            # shuffle for making use of BN\n",
    "            im_k_, idx_unshuffle = self._batch_shuffle_single_gpu(im_k)\n",
    "\n",
    "            k = self.encoder_k(im_k_)  # keys: NxC\n",
    "            k = nn.functional.normalize(k, dim=1)  # already normalized\n",
    "\n",
    "            # undo shuffle\n",
    "            k = self._batch_unshuffle_single_gpu(k, idx_unshuffle)\n",
    "\n",
    "        # compute logits\n",
    "        # Einstein sum is more intuitive\n",
    "        # positive logits: Nx1\n",
    "        l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)\n",
    "        # negative logits: NxK\n",
    "        l_neg = torch.einsum('nc,ck->nk', [q, self.queue.clone().detach()])\n",
    "\n",
    "        # logits: Nx(1+K)\n",
    "        logits = torch.cat([l_pos, l_neg], dim=1)\n",
    "\n",
    "        # apply temperature\n",
    "        logits /= self.T\n",
    "\n",
    "        # labels: positive key indicators\n",
    "        labels = torch.zeros(logits.shape[0], dtype=torch.long).cuda()\n",
    "        \n",
    "        loss = nn.CrossEntropyLoss().cuda()(logits, labels)\n",
    "\n",
    "        return loss, q, k, logits, labels\n",
    "\n",
    "    def forward(self, im1, im2):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            im_q: a batch of query images\n",
    "            im_k: a batch of key images\n",
    "        Output:\n",
    "            loss\n",
    "        \"\"\"\n",
    "\n",
    "        # update the key encoder\n",
    "        with torch.no_grad():  # no gradient to keys\n",
    "            self._momentum_update_key_encoder()\n",
    "\n",
    "        # compute loss\n",
    "        if self.symmetric:  # asymmetric loss\n",
    "            loss_12, _, k2, logits2, labels2 = self.contrastive_loss(im1, im2)\n",
    "            loss_21, _, k1, logits1, labels1 = self.contrastive_loss(im2, im1)\n",
    "            loss = loss_12 + loss_21\n",
    "            k = torch.cat([k1, k2], dim=0)\n",
    "            logits = torch.cat([logits1, logits2], dim=0)\n",
    "            labels = torch.cat([labels1, labels2], dim=0)\n",
    "        else:  # asymmetric loss\n",
    "            loss, _, k, logits, labels = self.contrastive_loss(im1, im2)\n",
    "\n",
    "        self._dequeue_and_enqueue(k)\n",
    "\n",
    "        return loss, logits, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb5631261d0f6bd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training SimCLR\n",
    "\n",
    "Train the SimCLR model using the contrastive loss and augmented image pairs from CIFAR-10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with gpu: cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [00:52<00:00,  1.84it/s]\n",
      "Feature extracting: 100%|██████████| 98/98 [00:05<00:00, 17.34it/s]\n",
      "Test Epoch: [0/200] Acc@1:21.98%: 100%|██████████| 20/20 [00:01<00:00, 11.59it/s]\n",
      "100%|██████████| 97/97 [00:50<00:00,  1.92it/s]\n",
      " 68%|██████▊   | 66/97 [00:34<00:15,  1.95it/s]"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from utils import accuracy, save_checkpoint\n",
    "# test using a knn monitor\n",
    "def test(net, memory_data_loader, test_data_loader, epoch):\n",
    "    net.eval()\n",
    "    classes = len(memory_data_loader.dataset.classes)\n",
    "    total_top1, total_top5, total_num, feature_bank = 0.0, 0.0, 0, []\n",
    "    with torch.no_grad():\n",
    "        # generate feature bank\n",
    "        for data, target in tqdm(memory_data_loader, desc='Feature extracting'):\n",
    "            feature = net(data.cuda(non_blocking=True))\n",
    "            feature = F.normalize(feature, dim=1)\n",
    "            feature_bank.append(feature)\n",
    "        # [D, N]\n",
    "        feature_bank = torch.cat(feature_bank, dim=0).t().contiguous()\n",
    "        # [N]\n",
    "        feature_labels = torch.tensor(memory_data_loader.dataset.targets, device=feature_bank.device)\n",
    "        # loop test data to predict the label by weighted knn search\n",
    "        test_bar = tqdm(test_data_loader)\n",
    "        for data, target in test_bar:\n",
    "            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
    "            feature = net(data)\n",
    "            feature = F.normalize(feature, dim=1)\n",
    "\n",
    "            pred_labels = knn_predict(feature, feature_bank, feature_labels, classes, 200, 0.1)\n",
    "\n",
    "            total_num += data.size(0)\n",
    "            total_top1 += (pred_labels[:, 0] == target).float().sum().item()\n",
    "            test_bar.set_description('Test Epoch: [{}/{}] Acc@1:{:.2f}%'.format(epoch, 200, total_top1 / total_num * 100))\n",
    "\n",
    "    return total_top1 / total_num * 100\n",
    "\n",
    "# knn monitor as in InstDisc https://arxiv.org/abs/1805.01978\n",
    "# implementation follows http://github.com/zhirongw/lemniscate.pytorch and https://github.com/leftthomas/SimCLR\n",
    "def knn_predict(feature, feature_bank, feature_labels, classes, knn_k, knn_t):\n",
    "    # compute cos similarity between each feature vector and feature bank ---> [B, N]\n",
    "    sim_matrix = torch.mm(feature, feature_bank)\n",
    "    # [B, K]\n",
    "    sim_weight, sim_indices = sim_matrix.topk(k=knn_k, dim=-1)\n",
    "    # [B, K]\n",
    "    sim_labels = torch.gather(feature_labels.expand(feature.size(0), -1), dim=-1, index=sim_indices)\n",
    "    sim_weight = (sim_weight / knn_t).exp()\n",
    "\n",
    "    # counts for each class\n",
    "    one_hot_label = torch.zeros(feature.size(0) * knn_k, classes, device=sim_labels.device)\n",
    "    # [B*K, C]\n",
    "    one_hot_label = one_hot_label.scatter(dim=-1, index=sim_labels.view(-1, 1), value=1.0)\n",
    "    # weighted score ---> [B, C]\n",
    "    pred_scores = torch.sum(one_hot_label.view(feature.size(0), -1, classes) * sim_weight.unsqueeze(dim=-1), dim=1)\n",
    "\n",
    "    pred_labels = pred_scores.argsort(dim=-1, descending=True)\n",
    "    return pred_labels\n",
    "\n",
    "with Timer():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training with gpu: {device}.\")\n",
    "    # Initialize optimizer and loss criterion\n",
    "    model = ModelMoCo(\n",
    "            dim=128,\n",
    "            K=4096,\n",
    "            m=0.99,\n",
    "            T=0.1,\n",
    "            arch='resnet18',\n",
    "            bn_splits=1,\n",
    "            symmetric=True,)\n",
    "    model = model.to(device)\n",
    "    lr = 6e-3\n",
    "    weight_decay = 5e-4\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader), eta_min=0,\n",
    "                                                               last_epoch=-1)\n",
    "    writer = SummaryWriter()\n",
    "    logging.basicConfig(filename=os.path.join(writer.log_dir, 'training.log'), level=logging.DEBUG)\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    # Set number of training epochs\n",
    "    epochs = 200\n",
    "    log_every_n_epochs = 10\n",
    "    logging.info(f\"Start SimCLR training for {epochs} epochs.\")\n",
    "    logging.info(f\"Training with gpu: {device}.\")\n",
    "    best_acc = 0\n",
    "    for epoch_counter in range(epochs):\n",
    "        loss_epoch = 0\n",
    "        for images, _ in tqdm(train_loader):\n",
    "            im_1, im_2 = images\n",
    "            im_1, im_2 = im_1.cuda(non_blocking=True), im_2.cuda(non_blocking=True)\n",
    "\n",
    "            loss, logits, labels = model(im_1, im_2)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_epoch += loss.item()\n",
    "            # scaler.scale(loss).backward()\n",
    "            # scaler.step(self.optimizer)\n",
    "            # scaler.update()\n",
    "        avg_loss = loss_epoch / len(train_loader)\n",
    "        # print(f\"Epoch {epoch_counter}:\\tLoss: {avg_loss}\")\n",
    "        # every log_every_n_epochs log epoch loss and accuracy\n",
    "        if epoch_counter % log_every_n_epochs == 0:\n",
    "            top1, top5 = accuracy(logits, labels, topk=(1, 5))\n",
    "            test_acc_1_knn = test(model.encoder_q, memory_loader, test_loader, epoch_counter)\n",
    "            writer.add_scalar('loss', avg_loss, global_step=epoch_counter)\n",
    "            writer.add_scalar('acc/top1', top1[0], global_step=epoch_counter)\n",
    "            writer.add_scalar('acc/top5', top5[0], global_step=epoch_counter)\n",
    "            writer.add_scalar('learning_rate', scheduler.get_last_lr()[0], global_step=epoch_counter)\n",
    "            writer.add_scalar('test_acc_1_knn', test_acc_1_knn, global_step=epoch_counter)\n",
    "            if top1[0] > best_acc:\n",
    "                best_acc = top1[0]\n",
    "                save_checkpoint({\n",
    "                    'epoch': epoch_counter,\n",
    "                    'arch': 'resnet18',\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                }, is_best=True, filename=os.path.join(writer.log_dir, f'checkpoint_best.pth.tar'))\n",
    "\n",
    "\n",
    "        # warmup for the first 10 epochs\n",
    "        if epoch_counter >= 10:\n",
    "            scheduler.step()\n",
    "        logging.debug(f\"Epoch: {epoch_counter}\\tLoss: {loss}\\tTop1 accuracy: {top1[0]}\")\n",
    "\n",
    "    logging.info(\"Training has finished.\")\n",
    "    # save model checkpoints\n",
    "    checkpoint_name = 'checkpoint_{:04d}.pth.tar'.format(epochs)\n",
    "    save_checkpoint({\n",
    "        'epoch': epochs,\n",
    "        'arch': 'resnet18',\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best=False, filename=os.path.join(writer.log_dir, checkpoint_name))\n",
    "    logging.info(f\"Model checkpoint and metadata has been saved at {writer.log_dir}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-31T14:44:05.306675242Z"
    }
   },
   "id": "5b7f76864c04709f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the model checkpoint and evaluate the learned representations using Linear Probing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7a1fc915ff1ad65"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.resnet18(pretrained=False, num_classes=10).to(device)\n",
    "# Load the checkpoint\n",
    "checkpoint_path = 'runs/Sep26_17-15-26_cpsadmin-Z790-AORUS-ELITE-AX/checkpoint_best.pth.tar'\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "state_dict = checkpoint['state_dict']\n",
    "# model.load_state_dict(state_dict)\n",
    "\n",
    "for k in list(state_dict.keys()):\n",
    "  if k.startswith('backbone.'):\n",
    "    if k.startswith('backbone') and not k.startswith('backbone.fc'):\n",
    "      # remove prefix\n",
    "      state_dict[k[len(\"backbone.\"):]] = state_dict[k]\n",
    "  del state_dict[k]\n",
    "log = model.load_state_dict(state_dict, strict=False)\n",
    "assert log.missing_keys == ['fc.weight', 'fc.bias']"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "974bc2c895cb2485"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# freeze all layers but the last fc\n",
    "for name, param in model.named_parameters():\n",
    "    if name not in ['fc.weight', 'fc.bias']:\n",
    "        param.requires_grad = False\n",
    "\n",
    "parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "assert len(parameters) == 2  # fc.weight, fc.bias"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "93f72ecf304a489f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "def get_cifar10_data_loaders(download, shuffle=False, batch_size=256):\n",
    "  train_dataset = datasets.CIFAR10('./data', train=True, download=download,\n",
    "                                  transform=transforms.ToTensor())\n",
    "\n",
    "  train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                            num_workers=8, drop_last=False, shuffle=shuffle)\n",
    "  \n",
    "  test_dataset = datasets.CIFAR10('./data', train=False, download=download,\n",
    "                                  transform=transforms.ToTensor())\n",
    "    \n",
    "  test_loader = DataLoader(test_dataset, batch_size=2*batch_size,\n",
    "                            num_workers=8, drop_last=False, shuffle=shuffle)\n",
    "  return train_loader, test_loader\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "6e880c61a3136e20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=0.0008)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "train_loader, test_loader = get_cifar10_data_loaders(download=True)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a79946219b2fb147"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import accuracy\n",
    "epochs = 10\n",
    "with Timer():\n",
    "    for epoch in range(epochs):\n",
    "        top1_train_accuracy = 0\n",
    "        for counter, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            logits = model(x_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            top1 = accuracy(logits, y_batch, topk=(1,))\n",
    "            top1_train_accuracy += top1[0]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        top1_train_accuracy /= (counter + 1)\n",
    "        top1_accuracy = 0\n",
    "        top5_accuracy = 0\n",
    "        for counter, (x_batch, y_batch) in enumerate(test_loader):\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            logits = model(x_batch)\n",
    "            \n",
    "            top1, top5 = accuracy(logits, y_batch, topk=(1,5))\n",
    "            top1_accuracy += top1[0]\n",
    "            top5_accuracy += top5[0]\n",
    "        \n",
    "        top1_accuracy /= (counter + 1)\n",
    "        top5_accuracy /= (counter + 1)\n",
    "        print(f\"Epoch {epoch}:\\tTrain Accuracy: {top1_train_accuracy.item():.2f}\\tTest Accuracy: {top1_accuracy.item():.2f}\\tTest Top-5 Accuracy: {top5_accuracy.item():.2f}\")\n",
    "  \n",
    "  "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "39c86d430700e100"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train a ResNet18 model from scratch on CIFAR-10 using the sane augmentation strategy as SimCLR  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "855fe8cc26f958af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "def get_cifar10_data_loaders(download, shuffle=False, batch_size=256):\n",
    "  train_dataset = datasets.CIFAR10('./data', train=True, download=download,\n",
    "                                  transform=transforms.ToTensor())\n",
    "\n",
    "  train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                            num_workers=0, drop_last=False, shuffle=shuffle)\n",
    "  \n",
    "  test_dataset = datasets.CIFAR10('./data', train=False, download=download,\n",
    "                                  transform=transforms.ToTensor())\n",
    "\n",
    "  test_loader = DataLoader(test_dataset, batch_size=2*batch_size,\n",
    "                            num_workers=10, drop_last=False, shuffle=shuffle)\n",
    "  return train_loader, test_loader\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "74586d7d4739fb2d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "model = resnet18(pretrained=False, num_classes=10).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=0.0008)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "train_loader, test_loader = get_cifar10_data_loaders(download=True)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "8ab2d774394f9728"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import accuracy\n",
    "epochs = 10\n",
    "with Timer():\n",
    "    for epoch in range(epochs):\n",
    "        top1_train_accuracy_sup = 0\n",
    "        for counter, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            logits = model(x_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            top1 = accuracy(logits, y_batch, topk=(1,))\n",
    "            top1_train_accuracy_sup += top1[0]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        top1_train_accuracy_sup /= (counter + 1)\n",
    "        top1_accuracy_sup = 0\n",
    "        top5_accuracy_sup = 0\n",
    "        for counter, (x_batch, y_batch) in enumerate(test_loader):\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            logits = model(x_batch)\n",
    "            \n",
    "            top1, top5 = accuracy(logits, y_batch, topk=(1,5))\n",
    "            top1_accuracy_sup += top1[0]\n",
    "            top5_accuracy_sup += top5[0]\n",
    "        \n",
    "        top1_accuracy_sup /= (counter + 1)\n",
    "        top5_accuracy_sup /= (counter + 1)\n",
    "        print(f\"Epoch {epoch}:\\tTrain Accuracy: {top1_train_accuracy_sup.item():.2f}\\tTest Accuracy: {top1_accuracy_sup.item():.2f}\\tTest Top-5 Accuracy: {top5_accuracy_sup.item():.2f}\")\n",
    "            "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "9dcfa7b15c8395e7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = resnet18(pretrained=True).to(device)\n",
    "# overwrite the last fc layer\n",
    "model.fc = nn.Linear(512, 10).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=0.0008)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "train_loader, test_loader = get_cifar10_data_loaders(download=True)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "d3d787cb5d3d66dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import accuracy\n",
    "epochs = 10\n",
    "with Timer():\n",
    "    for epoch in range(epochs):\n",
    "        top1_train_accuracy_sup_pre = 0\n",
    "        for counter, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            logits = model(x_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            top1 = accuracy(logits, y_batch, topk=(1,))\n",
    "            top1_train_accuracy_sup_pre += top1[0]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        top1_train_accuracy_sup_pre /= (counter + 1)\n",
    "        top1_accuracy_sup_pre = 0\n",
    "        top5_accuracy_sup_pre = 0\n",
    "        for counter, (x_batch, y_batch) in enumerate(test_loader):\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            logits = model(x_batch)\n",
    "            \n",
    "            top1, top5 = accuracy(logits, y_batch, topk=(1,5))\n",
    "            top1_accuracy_sup_pre += top1[0]\n",
    "            top5_accuracy_sup_pre += top5[0]\n",
    "        \n",
    "        top1_accuracy_sup_pre /= (counter + 1)\n",
    "        top5_accuracy_sup_pre /= (counter + 1)\n",
    "        # print every 10 epochs\n",
    "        print(f\"Epoch {epoch}:\\tTrain Accuracy: {top1_train_accuracy_sup_pre.item():.2f}\\tTest Accuracy: {top1_accuracy_sup_pre.item():.2f}\\tTest Top-5 Accuracy: {top5_accuracy_sup_pre.item():.2f}\")\n",
    "            "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ddac36785f993370"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print the results of the SimCLR model and the supervised model in a table format\n",
    "print(f\"{'Model':<25}{'Train Accuracy':<20}{'Test Accuracy':<20}{'Test Top-5 Accuracy':<20}\")\n",
    "print(f\"{'SimCLR':<25}{top1_train_accuracy.item():<20.2f}{top1_accuracy.item():<20.2f}{top5_accuracy.item():<20.2f}\")\n",
    "print(f\"{'Supervised':<25}{top1_train_accuracy_sup.item():<20.2f}{top1_accuracy_sup.item():<20.2f}{top5_accuracy_sup.item():<20.2f}\")\n",
    "print(f\"{'Supervised Pretrained':<25}{top1_train_accuracy_sup_pre.item():<20.2f}{top1_accuracy_sup_pre.item():<20.2f}{top5_accuracy_sup_pre.item():<20.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "692dd08a956a4246"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8676515f2c730790"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
